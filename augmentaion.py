from imblearn.over_sampling import SMOTE
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
import pickle
import os
import numpy as np

# Fixer les graines pour la reproductibilitÃ©
np.random.seed(42)
tf.random.set_seed(42)

# Charger les donnÃ©es
file_path = "DatasetmalwareExtrait.csv"
if not os.path.exists(file_path):
    raise FileNotFoundError(f"Le fichier {file_path} est introuvable.")

data = pd.read_csv(file_path)

# VÃ©rification des colonnes nÃ©cessaires
required_columns = ['legitimate']
if 'legitimate' not in data.columns:
    raise ValueError("La colonne 'legitimate' est manquante dans le dataset.")

# SÃ©paration des features et de la cible
X = data.drop('legitimate', axis=1)
y = data['legitimate']

# Application de SMOTE pour Ã©quilibrer les classes
print("âš™ï¸ Application de SMOTE...")
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Afficher la distribution aprÃ¨s SMOTE
print("ğŸ”„ RÃ©partition aprÃ¨s SMOTE :")
print(pd.Series(y_resampled).value_counts())

# Normalisation des donnÃ©es aprÃ¨s SMOTE
columns_to_normalize = ['AddressOfEntryPoint', 'ResourceSize', 'SizeOfStackReserve']
scaler = StandardScaler()
try:
    X_resampled[columns_to_normalize] = scaler.fit_transform(X_resampled[columns_to_normalize])
except KeyError as e:
    raise ValueError(f"Erreur lors de la normalisation : {e}")

# Division des donnÃ©es Ã©quilibrÃ©es en train/test
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# Construction du modÃ¨le Keras
model = Sequential([
    Dense(64, activation='relu', input_dim=X_train.shape[1]),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# EntraÃ®nement du modÃ¨le
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)

# Ã‰valuation sur les donnÃ©es de test
y_pred = (model.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… PrÃ©cision sur les donnÃ©es de test : {accuracy:.2f}")
print("âœ… Rapport de classification :")
print(classification_report(y_test, y_pred))

# Sauvegarder le modÃ¨le et le scaler
model.save("model_augmented_keras.h5")
with open("scaler_augmented.pkl", "wb") as scaler_file:
    pickle.dump(scaler, scaler_file)

# Sauvegarder le dataset augmentÃ©
augmented_path = "dataset_augmented.csv"
pd.concat([X_resampled, pd.Series(y_resampled, name='legitimate')], axis=1).to_csv(augmented_path, index=False)
print(f"âœ… Dataset augmentÃ© sauvegardÃ© sous '{augmented_path}'.")

print("ğŸ¯ ModÃ¨le rÃ©-entraÃ®nÃ© et sauvegardÃ© avec succÃ¨s aprÃ¨s application de SMOTE.")
